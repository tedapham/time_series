---
title: "Lab3_tp_v1"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Unemployment Rate Data

## Exploratory Data Analysis
```{r}
library(Hmisc)
library(forecast)
library(tseries)
library(astsa)
library(vars)

# Set working directory, change to appropriate 
path <- "D:/CloudStation/W271 Time Series/HWs and Labs/Lab3"
setwd(path)
df <- read.csv("UNRATENSA.csv", stringsAsFactors = FALSE)
summary(df)
describe(df)
```

The unemployment rate data spans from January 1948 to June 2017, accounting for 834 months. There was no missing value in the data.
The lowest and higest are within range [2.4,11.4] corresponding to `r df$DATE[which.min(df$UNRATENSA)]`
and `r df$DATE[which.max(df$UNRATENSA)]`.


### Transform the dataset to time series object
```{r}
#Function to print the EDA plots for a time series
print_tsplots <- function(ts_data) {
    par(mfrow=c(2,2))
    plot(ts_data)
    acf(ts_data)
    pacf(ts_data)
    monthplot(ts_data)
}

unratensa <- ts(df$UNRATENSA, frequency = 12, start = c(1948,1))
unrate.short <- window(unratensa,start = c(1976,1))


par(mfrow=c(1,2))
hist(unratensa)
hist(unrate.short)
```

```{r}
par(mfrow=c(2,1))

plot(unratensa)
plot(unrate.short)
```


Between 1948 and 1970, the unemployment rate has a lower variance compared to 1970 to present. The lowest rate 2.1 was in October 1953 which seemed unreasonably low in this date and age. 1970 was when the first personal computer was introduced and since then there have been periods of technology disruptions and hence more volatile changes in the labor market. Today current working environment are facing similar trends of technology replacing labor workforce, economic downturn. We decide to include only 1970 data onward.


### Trend and Seasonality


```{r}

##Add Smoothing to see if there's linear trend
```




```{r}

#Split the data into 
emp.training <- window(unratensa, start = c(1976,1), end = c(2014,12))
emp.test <- window(unratensa, start = c(2015,1))

```


Take log of the data because we think XXXX
```{r}
emp.test.log <- log(emp.test)
emp.log <- log(emp.training)
print_tsplots(emp.log)

```
The transformed series is not stationary in the mean.



Take 1st difference XXX
```{r}

emp.yd <- diff(emp.training, lag = 12)
print_tsplots(emp.yd)

```

```{r}
adf.test(emp.yd)
pp.test(emp.yd)
```


The unit root test was not significant



```{r}
emp.yd.log <- diff(log(emp.training), lag = 12)
print_tsplots(emp.yd.log)
```
```{r}

adf.test(emp.yd.log)
pp.test(emp.yd.log)
```


The unit root test again fails to rejet the null hypothesis.

```{r}
summary(emp.yd.log)
```

From the ACF plot, a gradual decrease of ACF over time indicates trends. We also observe sporadic uptick at lags 1, 2, 3, which is evident of seasonality.
The trend and seasonality must be omitted from 
Add intepretation for the month plot XXX


```{r}
```

```{r}
```

### Transformation to Stationary Question 1:
During your EDA, you notice that your data exhibits both seasonality (different months have different heights) AND that there is a clear linear trend. How many order of non-seasonal and seasonal differencing would it take to make this time-series stationary in the mean? Why? XXX

Where is clear linear trend?



```{r}


```

The pacf still have values outside of the boundaries, first order difference does not completely render the series stationary.
XXX need more modeling

##Modeling
```{r}

# define get best arima function
get.best.arima <- function(ts,p,q,P,Q)
{
  #initialize best.aic to a large number 
  best.aic <- 1e8
  for (p.i in 0:p ) for (q.i in 0:q)
    for (P.i in 0:P)  for (Q.i in 0:Q)
    {
      print(c(p.i,q.i,P.i,Q.i))
      try(fit <-Arima(emp.training, order = c(p.i, 1, q.i), seasonal = list(order = c(P.i,0, Q.i)), method = "ML"))

      fit.aic <- fit$aic
      #fit.aic <- -2*fit$loglik + (log(n)+1)*length(fit$coef)
      if (fit.aic < best.aic)
      {
        best.aic <- fit.aic
        best.fit <-fit
        best.model <- c(p.i,1,q.i,P.i,0,Q.i)
      }
    }
  list(best.aic,best.fit,best.model)
}


get.best.arima(emp.log,4,4,0,0)
```
For non Seasonal, the best model is 4,1,4


```{r}
get.best.arima(emp.training, 4,4,1,1)
```

## Forecasting

```{r}
### Check whether differencing is needed and its impact on the model
# m1 <- Arima(emp.training, order = c(3, 2, 1), seasonal = list(order = c(0, 1, 2)))
# m2 <- Arima(emp.training, order = c(3, 2, 2), seasonal = list(order = c(0, 1, 2)))
# m3 <- Arima(emp.training, order = c(3, 2, 3), seasonal = list(order = c(0, 1, 2)))
# m4 <- Arima(log(emp.training), order = c(3, 2, 2), seasonal = list(order = c(0, 1, 2)))

m1 <- Arima(emp.log, order = c(1, 1, 2), seasonal = list(order = c(1, 1, 1)))
m2 <- Arima(emp.log, order = c(1, 1, 2), seasonal = list(order = c(1, 1, 2)))
m3 <- Arima(emp.log, order = c(2, 1, 2), seasonal = list(order = c(1, 1, 1)))
m4 <- Arima(emp.log, order = c(2, 1, 2), seasonal = list(order = c(1, 1, 2)))
forecast1 <- forecast(m1, h = length(emp.test.log))
forecast2 <- forecast(m2, h = length(emp.test.log))
forecast3 <- forecast(m3, h = length(emp.test.log))
forecast4 <- forecast(m4, h = length(emp.test.log))

ts.plot(emp.test.log,forecast1$mean,forecast2$mean,forecast3$mean,forecast4$mean, 
        gpars=list(xlab="year", ylab="Unemployment Rate"), 
        col = c("black", "red", "blue", "green", "orange"), lwd = 2, lty = 1:5)

# lines(forecast1$mean, col = "red", lwd=2)
# lines(forecast2$mean, col = "blue", lwd=2)
# lines(forecast3$mean, col = "green", lwd=2)

#lines(forecast4$mean, col = "green")

par(mfrow=c(2,2))
plot(m1$residuals)
plot(m2$residuals)
plot(m3$residuals)
plot(m4$residuals)

# adf.test(m1$residual)
# adf.test(m2$residual)
# adf.test(m3$residual)
# adf.test(m4$residual)

summary(m1)
summary(m2)
summary(m3)
summary(m4)
#lines(forecast3$mean, col = "green")

```

1. How well does your model predict the unemployment rate up until June 2017?


2. What does the unemployment rate look like at the end of 2020? How credible is this estimate?

##(B) Build a linear time-regression and incorporate seasonal effects. Be sure to evaluate the residuals and assess this model on the basis of the assumptions of the classical linear model, and then produce a 1 year and a 4 year forecast.
1/ How well does your model predict the unemployment rate up until June 2017?
2/ What does the unemployment rate look like at the end of 2020? How credible is this estimate?
3/ Compare this forecast to the one produced by the SARIMA model. What do you notice?



# Autosale Data


##EDA

XXX Need to clean up variable names

```{r}
df3 <- read.csv("TOTALNSA.csv", stringsAsFactors = FALSE)
head(df3)
tail(df3)
describe(df3)

hist(df3$TOTALNSA)
totalnsa <- ts(df3$TOTALNSA, frequency = 12, start = c(1976,1))
plot(totalnsa)
acf(totalnsa, lag.max = 480)
pacf(totalnsa, lag.max = 480)
monthplot(totalnsa)

```



##Modeling

#Question 3: VAR
You also have data on automotive car sales. Use a VAR model to produce a 1 year forecast on both the unemployment rate and automotive sales for 2017 in the US.
Compare the 1 year forecast for unemployment produced by the VAR and SARIMA models, examining both the accuracy AND variance of the forecast. Do you think the addition of the automotive sales data helps? Why or why not?

##EDA to select the order of the VAR model
```{r}
df3 <- read.csv("TOTALNSA.csv", stringsAsFactors = FALSE)
head(df3)
tail(df3)
#describe(df3)

totalnsa <- ts(df2$UNRATENSA, frequency = 12, start = c(1976,1), end = c(2017, 6))
par(mfrow=c(2,2))
print_tsplots(totalnsa)

tnsa.training <- window(totalnsa, start = c(1976,1), end = c(2014,12))
tnsa.test <- window(totalnsa, start = c(2015,1))
tnsa.log <- log(tnsa.training)

adf.test(tnsa.log)

adf.test(emp.log)

empnsa <- cbind(emp.log, tnsa.log)
plot.ts(empnsa, main="Unemployment Rate - National Automotive Sales")

par(mfrow=c(1,2))
ts.plot(emp.log, tnsa.log, gpars = list(xlab = "Time series period - 2015 - 2017", ylab = "Time series values", lty = c(1:3), pch = c(1,4), col = c("black", "blue")))
ts.plot(window(emp.test.log, start = c(2017,1)), window(log(tnsa.test), start = c(2017,1)), gpars = list(xlab = "Time series period-2017", ylab = "Time series values", lty = c(1:3), pch = c(1,4), col = c("black", "blue")))
po.test(empnsa)
VARselect(empnsa, lag.max = 36, type = "both")
```

##Build the VAR model using order=25
```{r}
empnsa.var <- VAR(empnsa, p=25, type = "trend")
coef(empnsa.var)
par(mfrow=c(1,2))
acf(resid(empnsa.var)[, 1])
acf(resid(empnsa.var)[, 2])

empnsa.pred <- predict(empnsa.var, n.ahead = length(tnsa.test))
empnsa.pred
emp.log.pred <- ts(empnsa.pred$fcst$emp.log[,1], st = c(2015,1), fr=12)
tnsa.log.pred <- ts(empnsa.pred$fcst$tnsa.log[,1], st = c(2015,1), fr=12)
ts.plot(log(tnsa.test), type = 'l')
lines(tnsa.log.pred, type = 'l', col = 'red')

ts.plot(emp.test.log, type = 'l')
lines(emp.log.pred, type = 'l', col = 'blue')
```


