---
title: "Lab3_tp_v1"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Unemployment Rate Data

## Exploratory Data Analysis
```{r}
library(Hmisc)
library(forecast)
library(tseries)
library(astsa)
library(vars)
library(psych)
library(zoo)
# Set working directory, change to appropriate 
#path <- "D:/CloudStation/W271 Time Series/HWs and Labs/Lab3"
#path <- "~/Documents/W271 Time Series/HWs and Labs/Lab3"
path <- "/Users/jayashreeraman/Dropbox/Lab3"
setwd(path)
df <- read.csv("UNRATENSA.csv", stringsAsFactors = FALSE)
summary(df)
describe(df)
```

The unemployment rate data spans from January 1948 to June 2017, accounting for 834 months. There was no missing value in the data.
The lowest and higest are within range [2.4,11.4] corresponding to `r df$DATE[which.min(df$UNRATENSA)]`
and `r df$DATE[which.max(df$UNRATENSA)]`.


### Determine which data to use
```{r}
unratensa <- ts(df$UNRATENSA, frequency = 12, start = c(1948,1))
unrate.short <- window(unratensa,start = c(1976,1))

par(mfrow=c(2,1))
plot(unratensa)
plot(unrate.short)
```

```{r}
par(mfrow=c(1,2))
hist(unratensa)
hist(unrate.short)
```
Between 1948 and 1970, the unemployment rate has a lower variance compared to 1970 to present. The lowest rate 2.1 was in October 1953 which seemed unreasonably low in this date and age. 1970 was when the first personal computer was introduced and since then there have been periods of technology disruptions and hence more volatile changes in the labor market. Today current working environment are facing similar trends of technology replacing labor workforce, economic downturn. In addition, because the data for autosale starts from 1975, we decide to include only 1970 data onward for consistency in other analyses.


### Trend and Seasonality

```{r}
k.smooth.wide <- ksmooth(time(unratensa), unratensa, kernel = c("normal"), bandwidth = 3) 
k.smooth.narrow <- ksmooth(time(unratensa), unratensa, kernel = c("normal"), bandwidth = 0.3)
plot(unratensa, col = 'gray')
lines(k.smooth.wide$x, k.smooth.wide$y, col =  'red' ) 
lines(k.smooth.narrow$x, k.smooth.narrow$y, col =  'blue' )
```
Using a narrow kernel smoother, we see evidence of seasonality in the blue line and the underlying trend in the red line.
We can further confirm the appearance of seasonality and trend with acf and pacf plots.


```{r}
acf(unratensa, lag.max = 72)
```


```{r}
pacf(unratensa, lag.max = 72)
```

From the acf, pacf plots, we see clear evidence of AR process (with pacf peak at 1 month lag 0.1 and gradual decrease of acf) and seasonality with pcaf peaked again at lag 1 = 12 month. 
From the ACF plot, a gradual decrease of ACF over time also indicates trends. We also observe sporadic uptick at lags 1, 2, 3, which is evident of seasonality every 12 months.

The trend and seasonality must be omitted from the time series 

### Transformation to stationary time series

```{r}

print_tsplots <- function(ts_data) {
par(mfrow=c(2,2))
plot(ts_data)
acf(ts_data,lag.max=72)
pacf(ts_data,lag.max=72)
monthplot(ts_data)
}

emp.training <- window(unratensa, start = c(1976,1), end = c(2014,12))
emp.test <- window(unratensa, start = c(2015,1))

print_tsplots(emp.training)
```
#### Transformation to Stationary Question 1:
During your EDA, you notice that your data exhibits both seasonality (different months have different heights) AND that there is a clear linear trend. How many order of non-seasonal and seasonal differencing would it take to make this time-series stationary in the mean? Why? 


From the time plot, we observe random walks in the data so it'd be reasonable to use first difference to stationary the data. Also from the adf plot, the data exhibits seasonality at 12 months (lag=1) so D can also be 1. 
Before we perform the difference, we need to determine if a non-linear transformation is necessary. To do this,
we examine the relationship between the trend and seasonality, specifically whether it's additive or multiplicative.

```{r}
par(mfrow=c(2,1))
plot(decompose(emp.training)$random)
plot(decompose(emp.training,type='multi')$random)

```

From the decomposition plots of the random component, it seems the multiplicative model for trend and seasonality is better because of a more constant variance.
Hence, a log transfomration would be useful here.


```{r}
emp.log <- log(emp.training)
print_tsplots(emp.log)

emp.test.log <- log(emp.test)
```

Now we take the difference to substract the random walk effect and deseason the data.

```{r}
emp.yd.log <- diff(diff(emp.log, lag = 12))
print_tsplots(emp.yd.log)
```

```{r}
adf.test(emp.log)
pp.test(emp.log)
adf.test(emp.yd.log)
pp.test(emp.yd.log)
```

We see from the above tests that we need one differencing for the seasonal lag = 12(D=1) and one difference for the non-seasonal lags(d=1) to make the series stationary - we will use these parameters later to create our SARIMA model

The unit root test and the adf.test both provide evidence  (p=0.01) to reject the null hypothesis that the transformed series is not stationary.


```{r}
summary(emp.log)
summary(emp.yd.log)
```

The pacf still have values outside of the boundaries, first order difference does not completely render the series stationary.

## Modeling

```{r}

# define get best arima function
get.best.arima <- function(ts,p,q,P,Q)
{
  #initialize best.aic to a large number 
  best.aic <- 1e8
   for (p.i in 0:p ) for (q.i in 0:q)
    for (P.i in 0:P)  for (Q.i in 0:Q)
    {
      try(fit <-Arima(emp.log, order = c(p.i,1, q.i), seasonal = list(order = c(P.i,1, Q.i)), method = "ML"))

      fit.aic <- fit$aic
      #fit.aic <- -2*fit$loglik + (log(n)+1)*length(fit$coef)
      if (fit.aic < best.aic)
      {
        best.aic <- fit.aic
        best.fit <-fit
        best.model <- c(p.i,1,q.i,P.i,1,Q.i)
        print(c(p.i,1,q.i,P.i,1,Q.i,best.aic))
      }
    }
  list(best.aic,best.fit,best.model)
}

```



```{r}
get.best.arima(emp.log,10,10,2,2)
```

## Forecasting

```{r}
m1 <- Arima(emp.log, order = c(1, 1, 2), seasonal = list(order = c(1, 1, 1)))

forecast1 <- forecast(m1, h = length(emp.test.log)+42)

ts.plot(emp.test.log,forecast1$mean, 
        gpars=list(xlab="year", ylab="Unemployment Rate"), 
        col = c("black", "red"), lwd = 2, lty = 1:2)

plot(m1$residuals)
df_fcst <- as.data.frame(forecast1)
tail(df_fcst)
```

1. How well does your model predict the unemployment rate up until June 2017?
```{r}
summary(m1)
hist(forecast1$residuals)
```


2. What does the unemployment rate look like at the end of 2020? How credible is this estimate?
```{r}
exp(df_fcst["Dec 2020",1])
```


##(B) Build a linear time-regression and incorporate seasonal effects. Be sure to evaluate the residuals and assess this model on the basis of the assumptions of the classical linear model, and then produce a 1 year and a 4 year forecast.

###Linear Regression model
```{r}
head(time(emp.training))
mnth = factor(cycle(emp.training))
lm_unemp <- tslm(emp.training ~ trend + season)
summary(lm_unemp)
```

###1. How well does your model predict the unemployment rate up until June 2017?
```{r}
lm_fcst<-forecast(lm_unemp, h=72)
hist(lm_fcst$residuals)
lm_fcst$mean
```

###2. What does the unemployment rate look like at the end of 2020? How credible is this estimate?
```{r}
lm_fcst$mean[72]
```

###3. Compare this forecast to the one produced by the SARIMA model. What do you notice?
```{r}
plot(lm_fcst)
lines(forecast1$mean, col = "brown")
```



# Autosale Data


##EDA

XXX Need to clean up variable names

```{r}
nsa <- read.csv("TOTALNSA.csv", stringsAsFactors = FALSE)
head(nsa)
tail(nsa)
describe(nsa)

totalnsa <- ts(nsa$TOTALNSA, frequency = 12, start = c(1976,1))
hist(totalnsa)

summary(totalnsa)

print_tsplots(totalnsa)
```

##Modeling

#Question 3: VAR
You also have data on automotive car sales. Use a VAR model to produce a 1 year forecast on both the unemployment rate and automotive sales for 2017 in the US.
Compare the 1 year forecast for unemployment produced by the VAR and SARIMA models, examining both the accuracy AND variance of the forecast. Do you think the addition of the automotive sales data helps? Why or why not?

##Modeling

##EDA to select the order of the VAR model - Stationarity of series
```{r}
tnsa.training <- window(totalnsa, start = c(1976,1), end = c(2014,12))
tnsa.test <- window(totalnsa, start = c(2015,1))

adf.test(tnsa.training)

adf.test(emp.log)

empnsa <- cbind(emp.log, tnsa.training)
plot.ts(empnsa, main="Unemployment Rate - National Automotive Sales")
```

The adf tests indicate that both the series are non stationary

##EDA to select the order of the VAR model - Co-integration of series
```{r}
po.test(empnsa)
```

The Philips-Oularis test clearly rejects the hypothesis that the series are cointegrated.
We can now proceed to select the order of the VAR model

##EDA to select the order of the VAR model - Use VARSelect
```{r}
VARselect(empnsa, lag.max = 36, type = "both")
```
We see here that the lowest AIC is for order=26

# ##Build the VAR model using order=26
```{r}
empnsa.var <- VAR(empnsa, p=26, type = "trend")
coef(empnsa.var)
par(mfrow=c(2,2))
hist(resid(empnsa.var)[, 1])
hist(resid(empnsa.var)[, 2])
acf(resid(empnsa.var)[, 1])
acf(resid(empnsa.var)[, 2])

empnsa.pred <- predict(empnsa.var, n.ahead = length(tnsa.test)+12)
empnsa.pred
emp.pred <- ts(empnsa.pred$fcst$emp.log[,1], st = c(2015,1), fr=12)
tnsa.pred <- ts(empnsa.pred$fcst$tnsa.training[,1], st = c(2015,1), fr=12)
ts.plot(tnsa.test, type = 'l')
lines(tnsa.pred, type = 'l', col = 'green')

ts.plot(emp.test.log, type = 'l')
lines(emp.pred, type = 'l', col = 'blue')
```

##Compare the 1 year forecast for unemployment produced by the VAR and SARIMA models, examining both the accuracy AND variance of the forecast. Do you think the addition of the automotive sales data helps? Why or why not?
```{r}
ts.plot(emp.test.log, window(forecast1$mean, end = c(2018,6)), emp.pred, 
        gpars=list(xlab="year", ylab="Unemployment Rate"), 
        col = c("black", "red", "blue"), lwd = 2, lty = 1:3)

accuracy(forecast1)
summary(emp.pred)
```


